# -*- coding: utf-8 -*-
"""GenAI-Assignment-2.3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10BzDdRzht7KELDFYrTbHrk4EnLAKx9dS
"""

# ðŸš€ Step 1: Install Required Packages

# ðŸš€ Step 2: Import Necessary Libraries
import faiss
import torch
import numpy as np
import gradio as gr
from transformers import AutoTokenizer, AutoModel
import json
import os
from datasets import load_dataset
import re
import nltk
from sentence_transformers import SentenceTransformer
import pickle

# Set file paths
job_descriptions_path = "job_descriptions.json"

def save_job_descriptions(dataset_name="cnamuangtoun/resume-job-description-fit"):
    print("ðŸ”„ Fetching job descriptions from Hugging Face...")
    dataset = load_dataset(dataset_name)

    # Extract job descriptions & remove duplicates
    job_descriptions = list(set(example["job_description_text"].strip() for example in dataset["train"]))

    # Save to file
    with open(job_descriptions_path, "w") as f:
        json.dump(job_descriptions, f)

# Run once to save job descriptions
save_job_descriptions()

def clean_text(text):
    """Removes excessive whitespace and normalizes text"""
    return re.sub(r'\s+', ' ', text).strip()

def extract_job_details(text):
    """Extracts structured job details using regex and better text processing."""
    text = clean_text(text)  # Preprocess text

    # Extract job title (refined pattern)
    title_match = re.search(r'(?i)(?:Position|Role|Job Title|Title)[:\s]+([^\n,|]+)', text)
    title = title_match.group(1).strip() if title_match else "Unknown Position"

    # Extract location (refined)
    location_match = re.search(r'(?i)(?:Location|Workplace|Work Mode)[:\s]+([^\n,|]+)', text)
    location = location_match.group(1).strip() if location_match else "Unknown Location"

    # Extract skills (better approach)
    skills_match = re.search(r'(?i)(?:Must Have Skills|Required Skills|Skills)[:\s]+([^\n]+)', text)
    skills = skills_match.group(1).strip() if skills_match else "Not Provided"

    # Extract experience (Handles "3-5 years", "3+ years", "3 years")
    experience_match = re.search(r'(\d{1,2}\+?)(?:\s*-\s*\d{1,2})?\s*years?\s*of experience', text, re.IGNORECASE)
    experience = experience_match.group(1).strip() if experience_match else "Not Specified"

    # Extract responsibilities (limit to 150 characters)
    sentences = nltk.sent_tokenize(text)
    responsibilities = " ".join(sentences[:2])[:150]  # Take first 2 sentences, limit length

    # Format cleaned job description
    cleaned_description = f"ðŸ“Œ {title} | {location}\nðŸ“– Responsibilities: {responsibilities}...\nðŸ”¹ Skills: {skills}\nðŸ—“ Experience: {experience}"

    return cleaned_description

# Load job descriptions from file
with open(job_descriptions_path, "r") as f:
    job_descriptions = json.load(f)

# Apply cleaning to all job descriptions
cleaned_descriptions = [extract_job_details(desc) for desc in job_descriptions]

# Load the sentence embedding model (adjust model if needed)
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Generate embeddings for all cleaned job descriptions
job_embeddings = embedding_model.encode(cleaned_descriptions, convert_to_numpy=True, normalize_embeddings=True)

# Save embeddings to FAISS index
embedding_dimension = job_embeddings.shape[1]  # Get embedding size
faiss_index = faiss.IndexFlatIP(embedding_dimension)  # Inner Product for similarity search
faiss_index.add(job_embeddings)  # Add embeddings to FAISS

# Save FAISS index and job descriptions for later use
faiss.write_index(faiss_index, "faiss_index.bin")
with open("job_descriptions.pkl", "wb") as f:
    pickle.dump(cleaned_descriptions, f)

def search_jobs(query, top_k=10, location="Chicago", min_experience=2, max_distance=0.6):
    # Convert the query into an embedding
    query_embedding = embedding_model.encode(query, convert_to_numpy=True)
    query_embedding = np.expand_dims(query_embedding, axis=0)  # Reshape for FAISS

    # Perform the FAISS search
    distances, indices = faiss_index.search(query_embedding, top_k * 2)  # Retrieve more to filter later

    # Extract the top job matches with additional filtering
    filtered_jobs = []
    for idx, distance in zip(indices[0], distances[0]):
        if distance > max_distance:  # Exclude weak matches
            continue

        job_desc = cleaned_descriptions[idx]

        # **Location Filtering**
        if location.lower() not in job_desc.lower():
            continue

        # **Experience Filtering** (Extract years of experience from description)
        exp_years = 0
        exp_match = re.search(r"(\d+)\+?\s*years?", job_desc, re.IGNORECASE)
        if exp_match:
            exp_years = int(exp_match.group(1))

        if exp_years < min_experience:
            continue  # Skip junior roles when looking for mid/senior positions

        filtered_jobs.append(job_desc)

        if len(filtered_jobs) >= top_k:
            break  # Stop once we get enough matches

    # Print the final matched jobs
    print("\nðŸ”Ž **Top Filtered Job Matches:**")
    for i, job in enumerate(filtered_jobs):
        print(f"\nðŸ“Œ Job {i+1}: {job}")
        print("-" * 50)

    return filtered_jobs

# Example queries with the new filtering
queries = [
    "Machine Learning Engineer with Python, SQL, and data visualization expertise. Seeking opportunities in Chicago."
]

# Run the improved search
for query in queries:
    search_jobs(query, top_k=1, location="Chicago", min_experience=2, max_distance=0.6)